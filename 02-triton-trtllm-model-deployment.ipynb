{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "323f65fd-c390-4b61-804f-e5cfcb70a7ef",
   "metadata": {},
   "source": [
    "<center>\n",
    "  <a href=\"https://escience.sdu.dk/index.php/ucloud/\">\n",
    "    <img src=\"https://escience.sdu.dk/wp-content/uploads/2020/03/logo_esc.svg\" width=\"400\" height=\"186\" />\n",
    "  </a>\n",
    "</center>\n",
    "<br>\n",
    "<p style=\"font-size: 1.2em;\">\n",
    "  This notebook was tested using <strong>Triton Inference Server (TRT-LLM) v25.02</strong> and machine type <code>u3-gpu4</code> on UCloud.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d7e1b0-6ab5-4f7c-bd2f-b51c916d6a90",
   "metadata": {},
   "source": [
    "# 02 - Deploying a Triton Inference Server for LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bb3216-f529-42d7-a4b5-700a8ffc0390",
   "metadata": {},
   "source": [
    "## üöÄ Introduction\n",
    "\n",
    "In this end‚Äëto‚Äëend tutorial, we‚Äôll take a pre-trained LLM (e.g., Llama 3.3 70B Instruct) and:\n",
    "1. Authenticate and download weights from Hugging Face.\n",
    "2. Convert the model into a TensorRT-LLM engine for high‚Äëperformance inference.\n",
    "3. Test the optimized engine locally.\n",
    "4. Package and deploy the engine on NVIDIA Triton Inference Server with inflight batching.\n",
    "5. Send sample requests to Triton and validate responses.\n",
    "6. Profile performance using Triton‚Äôs `genai-perf` tool.\n",
    "\n",
    "> üõ†Ô∏è **Important Environment Note:**\n",
    "> This notebook is designed to run on **UCloud**, using the **NVIDIA Triton Inference Server (TRT-LLM) app, version `v25.02`**.\n",
    "> If you encounter unexpected errors, **double-check you are using the correct app version**, and that your session includes **GPU resources**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0462d8-0a60-4c48-b8dd-06ae560be1dd",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 1: Hugging Face Authentication\n",
    "\n",
    "The following code creates a secure input widget for your Hugging Face token, which is required to authenticate and download the [Llama 3.3 70B Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) model from the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec322c9-9520-40bd-a75f-459ec0b6bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import Password\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "pwd = Password(description=\"Hugging Face Token:\")\n",
    "display(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcbe2c4-d9ce-4708-bbf4-479b56db40d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token = pwd.value\n",
    "hf_model=\"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "hf_model_path=\"models/llama-3.3/70B/hf\"\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=hf_model,\n",
    "    local_dir=hf_model_path,\n",
    "    token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb284d44-c990-4b16-b650-acb3e3be370d",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 2: Convert the Model Checkpoint to TensorRT-LLM Format\n",
    "\n",
    "The following Bash script sets up the required directories and executes the conversion of the Llama 3.3 70B Instruct model checkpoint from Hugging Face to a **TensorRT-ready** format for optimized performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ced79cc-20cb-43af-adec-fabd75697062",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "HF_MODEL=\"models/llama-3.3/70B/hf\"\n",
    "\n",
    "du -sh \"$HF_MODEL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc9a83-3659-4ce9-be7a-c077098d1511",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "HF_MODEL=\"models/llama-3.3/70B/hf\"\n",
    "TRT_CKPT=\"models/llama-3.3/70B/trt_ckpt/tp4\"\n",
    "mkdir -p \"$TRT_CKPT\"\n",
    "\n",
    "python -W ignore ~/llama/convert_checkpoint.py \\\n",
    "      --model_dir \"$HF_MODEL\" \\\n",
    "      --output_dir \"$TRT_CKPT\" \\\n",
    "      --dtype float16 \\\n",
    "      --tp_size 4 \\\n",
    "      --pp_size 1 \\\n",
    "      --load_by_shard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5411031-5aa0-4e27-935b-c847fb345a44",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 3: Build TensorRT-LLM Engine\n",
    "\n",
    "The following Bash script constructs the **TensorRT-LLM** engine from the previously converted Llama 3.3 70B Instruct model checkpoint. This optimization enhances the model's inference performance by leveraging TensorRT's efficient execution capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b55df1-420c-4530-bab0-b010efc6c9fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "TRT_CKPT=\"models/llama-3.3/70B/trt_ckpt/tp4\"\n",
    "TRT_ENGINE=\"models/llama-3.3/70B/trt_llm/tp4\"\n",
    "\n",
    "trtllm-build --checkpoint_dir \"$TRT_CKPT\" \\\n",
    "      --output_dir \"$TRT_ENGINE\" \\\n",
    "      --max_num_tokens 32768 \\\n",
    "      --max_input_len 4096  \\\n",
    "      --max_seq_len 8192 \\\n",
    "      --use_paged_context_fmha enable \\\n",
    "      --workers 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0734ff3-1f3e-49a6-a69b-534bf04ef9ae",
   "metadata": {},
   "source": [
    "### Checkpoint vs. Engine Directory Contents\n",
    "\n",
    "| Directory                                | Files                                                                    | Purpose                                                                                                                                                                                                                                   |\n",
    "|------------------------------------------|--------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| `models/llama-3.3/70B/trt_ckpt/tp4`      | `config.json`<br>`rank0.safetensors`<br>`rank1.safetensors`<br>`rank2.safetensors`<br>`rank3.safetensors` | **Intermediate checkpoint.**<br>- Each `rank*.safetensors` holds the raw FP16 weight shards for one tensor‚Äëparallel rank.<br>- `config.json` describes model dimensions, tokenizer settings, tensor‚Äëparallel layout, etc., for the builder. |\n",
    "| `models/llama-3.3/70B/trt_llm/tp4`       | `config.json`<br>`rank0.engine`<br>`rank1.engine`<br>`rank2.engine`<br>`rank3.engine`            | **Final TensorRT-LLM engine.**<br>- Each `*.engine` is a serialized, fully‚Äëfused CUDA kernel graph for one TP rank.<br>- Includes optimized GEMM/GELU fusions, memory‚Äëlayout transforms, and paged‚Äëcontext streaming (if enabled).      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15bf2f1-750f-4d4c-bff0-e4dd39a540c9",
   "metadata": {},
   "source": [
    "### Local Testing of TensorRT-Optimized model\n",
    "\n",
    "The following Bash script performs a local test of the optimized Llama 3.3 70B Instruct model. It sets the necessary environment variables and runs the `run.py` script with a sample prompt to evaluate the model's inference performance.\n",
    "\n",
    "If you get an error in the cell below, update the Transformer library:\n",
    "```bash\n",
    "pip install -U Transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eb42e3-6dc6-4692-a11e-d887dd8321ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "HF_MODEL=\"models/llama-3.3/70B/hf\"\n",
    "TRT_ENGINE=\"models/llama-3.3/70B/trt_llm/tp4\"\n",
    "\n",
    "PROMPT=\"What are the typical symptoms of an infected appendix?\"\n",
    "\n",
    "mpirun -n 4 python ~/run.py \\\n",
    "    --max_output_len=50 \\\n",
    "    --tokenizer_dir $HF_MODEL \\\n",
    "    --engine_dir $TRT_ENGINE \\\n",
    "    --input_text \"$PROMPT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ba3398-107e-4856-afc2-113070a1e808",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 4: Deploying Triton with Inflight Batching\n",
    "\n",
    "The following Bash scripts set up and configure Triton Inference Server for the Llama 3.3 70B Instruct model using **inflight batching**. This deployment optimizes inference performance by managing batch sizes and instance counts effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb947618-8ac0-4f98-b2ff-e2400cf28120",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "TRTITON_REPO=\"models/llama-3.3/70B/triton\"\n",
    "mkdir -p \"$TRTITON_REPO\"\n",
    "\n",
    "cp -r ~/all_models/inflight_batcher_llm/* \"$TRTITON_REPO\"\n",
    "\n",
    "ls \"$TRTITON_REPO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897a7902-effd-4235-a1a8-0dec875b9e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# preprocessing\n",
    "ENGINE_DIR=\"models/llama-3.3/70B/trt_llm/tp4\"\n",
    "TOKENIZER_DIR=\"models/llama-3.3/70B/hf\"\n",
    "MODEL_FOLDER=\"models/llama-3.3/70B/triton\"\n",
    "TRITON_MAX_BATCH_SIZE=16 # reduce this value if you encounter OOM cuda errors\n",
    "INSTANCE_COUNT=1 # one instance of the model, which is distributed on 4 GPUs\n",
    "MAX_QUEUE_DELAY_MS=1000 # Helps collect more requests into batches at very low overhead, or set to zero if you want lowest possible latency\n",
    "MAX_QUEUE_SIZE=512 # Allow some queuing under bursty load, or set to zero for pure online inferencing\n",
    "FILL_TEMPLATE_SCRIPT=\"$HOME/tools/fill_template.py\"\n",
    "LOGITS_DATA_TYPE=\"TYPE_FP32\"\n",
    "DECOUPLED_MODE=false # no decoupled streaming, matches inflight batching\n",
    "\n",
    "python ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/ensemble/config.pbtxt triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},logits_datatype:${LOGITS_DATA_TYPE}\n",
    "python ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/preprocessing/config.pbtxt tokenizer_dir:${TOKENIZER_DIR},triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},preprocessing_instance_count:${INSTANCE_COUNT}\n",
    "python ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/tensorrt_llm/config.pbtxt triton_backend:tensorrtllm,triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},decoupled_mode:${DECOUPLED_MODE},engine_dir:${ENGINE_DIR},max_queue_delay_microseconds:${MAX_QUEUE_DELAY_MS},batching_strategy:inflight_fused_batching,max_queue_size:${MAX_QUEUE_SIZE},logits_datatype:${LOGITS_DATA_TYPE},encoder_input_features_data_type:TYPE_FP16\n",
    "python ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/postprocessing/config.pbtxt tokenizer_dir:${TOKENIZER_DIR},triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},postprocessing_instance_count:${INSTANCE_COUNT},max_queue_size:${MAX_QUEUE_SIZE}\n",
    "python ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/tensorrt_llm_bls/config.pbtxt triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},decoupled_mode:${DECOUPLED_MODE},bls_instance_count:${INSTANCE_COUNT},logits_datatype:${LOGITS_DATA_TYPE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bb0716-5424-41e4-a5da-78d043fc7e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# The following Bash command starts an OpenAI-Compatible Frontend for Triton Inference Server\n",
    "# This is used to run predictions in Label Studio\n",
    "\n",
    "MODEL_FOLDER=\"models/llama-3.3/70B/triton\"\n",
    "TOKENIZER_DIR=\"models/llama-3.3/70B/hf\"\n",
    "\n",
    "stop_tritonserver\n",
    "\n",
    "nohup mpirun -np 4 python3 -W ignore /opt/tritonserver/python/openai/openai_frontend/main.py --model-repository $MODEL_FOLDER --backend tensorrtllm --tokenizer $TOKENIZER_DIR --openai-port 9000 --enable-kserve-frontends &> /work/triton-server-log.txt &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd45e251-b116-4b5e-80ba-4a183eb98914",
   "metadata": {},
   "source": [
    "The following Bash commands verify that the Triton server and the deployed Llama 3.3 70B Instruct model are running correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac0b462-23f2-4959-9c6d-b518b89c6138",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "LOG_FILE=\"/work/triton-server-log.txt\"\n",
    "\n",
    "# Function to wait for Triton to start by monitoring the log file\n",
    "wait_for_triton_start() {\n",
    "    echo \"Waiting for Triton Inference Server to start...\"\n",
    "    while true; do\n",
    "        # Check for all required startup messages\n",
    "        if grep -q 'Uvicorn running' \"$LOG_FILE\"; then\n",
    "                echo \"‚úÖ Uvicorn has started successfully.\"\n",
    "                break\n",
    "                \n",
    "        else\n",
    "            echo \"‚ùå Uvicorn has NOT started.. Retrying in 5 seconds...\"\n",
    "            sleep 5   \n",
    "        fi\n",
    "    done\n",
    "}\n",
    "\n",
    "# Wait for Triton to start\n",
    "wait_for_triton_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bc2515-138d-4b6c-894d-5a2043594bc2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "To test the model, run the following code in a terminal window:\n",
    "```bash\n",
    "MODEL=ensemble\n",
    "\n",
    "TEXT=\"An appendectomy is a surgical procedure to remove the appendix when it's infected. Symptoms typically include abdominal pain starting around the navel and shifting to the lower right abdomen, nausea, vomiting, and fever. Diagnosis is often made based on symptoms, physical examination, and imaging like ultrasound or CT scan. The procedure can be performed using open surgery or minimally invasive laparoscopic techniques. Recovery usually takes a few weeks, with minimal complications.\"\n",
    "\n",
    "curl -s http://localhost:9000/v1/chat/completions \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "        \"model\": \"'\"${MODEL}\"'\",\n",
    "        \"messages\": [\n",
    "          {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Given this medical text:\\n\\n'\"${TEXT}\"' \\n\\nGenerate direct, succinct, and unique medical questions covering symptoms, diagnosis, treatments, or patient management strategies. Output ONLY 5 questions clearly separated by new lines.\"\n",
    "          }\n",
    "        ],\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_tokens\": 500\n",
    "}' | jq -r '.choices[0].message.content'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501e523f-8fc2-4d59-a987-9ed0544e32a7",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 5: Performance Profiling with `genai-perf`\n",
    "\n",
    "To evaluate the performance of the deployed Llama 3.3 70B Instruct model on Triton Inference Server, execute the following Bash commands in a terminal session within Jupyter. This script uses [GenAI-Perf](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/perf_analyzer/genai-perf/README.html) to profile the model, generating performance metrics and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1235dcda-ca28-4890-bfbd-3a4c27ab84b7",
   "metadata": {},
   "source": [
    "- Each request simulates:\n",
    "    - An **input prompt** of around **200 tokens** (¬±10 tokens random noise)\n",
    "    - And expects **200 output tokens** exactly.\n",
    "- **1000 total prompts** per test, but each test measures for **10 seconds**.\n",
    "- **Concurrency** will sweep from **10** to **500** (500 requests *in flight* at the same time).\n",
    "- The input and output sizes are **large** ‚Äî total per request is ~400 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a1db57-d2c4-4e27-99ec-742236f24593",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Run the following code in a terminal window for a better output format\n",
    "\n",
    "TOKENIZER_DIR=\"models/llama-3.3/70B/hf\"\n",
    "\n",
    "export INPUT_PROMPTS=1000\n",
    "export INPUT_SEQUENCE_LENGTH=200\n",
    "export INPUT_SEQUENCE_STD=10\n",
    "export OUTPUT_SEQUENCE_LENGTH=200\n",
    "export MODEL=ensemble\n",
    "\n",
    "# Running multiple GenAI-Perf calls (the first for warm-up)\n",
    "for concurrency in 1 5 10 30 50 100 250 400 500; do\n",
    "    genai-perf profile -m $MODEL \\\n",
    "        --service-kind triton \\\n",
    "        --backend tensorrtllm \\\n",
    "        --num-prompts $INPUT_PROMPTS \\\n",
    "        --random-seed 1234 \\\n",
    "        --synthetic-input-tokens-mean $INPUT_SEQUENCE_LENGTH \\\n",
    "        --synthetic-input-tokens-stddev $INPUT_SEQUENCE_STD \\\n",
    "        --output-tokens-mean $OUTPUT_SEQUENCE_LENGTH \\\n",
    "        --output-tokens-stddev 0 \\\n",
    "        --output-tokens-mean-deterministic \\\n",
    "        --tokenizer $TOKENIZER_DIR \\\n",
    "        --concurrency $concurrency \\\n",
    "        --measurement-interval 25000 \\\n",
    "        --profile-export-file model_profile_${concurrency}.json \\\n",
    "        --url \"localhost:8001\" \\\n",
    "        --generate-plots\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cdc0fb-697d-4853-8605-ebedf5829c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "\n",
    "# Settings\n",
    "concurrency_levels = [10, 30, 50, 100, 250, 400, 500]  # Concurrency values you tested\n",
    "base_path = \"artifacts/ensemble-triton-tensorrtllm-concurrency{}\"\n",
    "\n",
    "# Storage for points\n",
    "x = []  # Time to first token (s)\n",
    "y = []  # Total system throughput (tokens/sec)\n",
    "labels = []  # Concurrency values\n",
    "\n",
    "# Read each JSON file\n",
    "for concurrency in concurrency_levels:\n",
    "    folder = base_path.format(concurrency)\n",
    "    json_path = os.path.join(folder, f\"model_profile_{concurrency}_genai_perf.json\")\n",
    "    \n",
    "    # Read JSON file\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Extract values\n",
    "    time_to_first_token_ms = data['time_to_first_token']['avg']  # milliseconds\n",
    "    output_token_throughput = data['output_token_throughput']['avg']  # tokens per second\n",
    "\n",
    "    # Append\n",
    "    x.append(time_to_first_token_ms / 1000)  # ms ‚Üí seconds\n",
    "    y.append(output_token_throughput)\n",
    "    labels.append(concurrency)\n",
    "\n",
    "\n",
    "# Now plot\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x, y=y,\n",
    "    mode='lines+markers+text',\n",
    "    text=labels,\n",
    "    textposition=\"middle center\",\n",
    "    line=dict(width=2),\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Time to first token (s)\",\n",
    "    yaxis_title=\"Throughput (tokens/s)\",\n",
    "    plot_bgcolor=\"rgba(240, 248, 255, 1)\",\n",
    "    font=dict(size=14),\n",
    "    width=800,\n",
    "    height=400,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
