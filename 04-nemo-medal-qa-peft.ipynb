{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f74ada-05b3-43da-b6de-c7e12745c66a",
   "metadata": {},
   "source": [
    "<center>\n",
    "  <a href=\"https://escience.sdu.dk/index.php/ucloud/\">\n",
    "    <img src=\"https://escience.sdu.dk/wp-content/uploads/2020/03/logo_esc.svg\" width=\"400\" height=\"186\" />\n",
    "  </a>\n",
    "</center>\n",
    "<br>\n",
    "<p style=\"font-size: 1.2em;\">\n",
    "  This notebook was tested using <strong>NeMo Framework v25.02.01</strong> and machine type <code>u3-gpu4</code> on UCloud.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3323204-1463-4df3-8c75-5e95b6d66ba1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 04 - Llama 3.1 Fine-tuning: Training on a Synthetic Medical Q&A Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a06833b-ba38-4deb-81d7-4fea58cd638f",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, our goal is to **fine-tune the Llama 3.1 Instruct model** for medical Q&A tasks.\n",
    "\n",
    "We will:\n",
    "- **Use a synthetic medical Q&A dataset** generated in [`03-ls-create-medal-qa-dataset.ipynb`](03-ls-create-medal-qa-dataset.ipynb).\n",
    "\n",
    "- **Start from a pre-converted NeMo checkpoint** of the Llama 3.1 Instruct model, prepared in [`01-nemo-medchat-qa-peft.ipynb`](01-nemo-medchat-qa-peft.ipynb)\n",
    "\n",
    "- **Apply LoRA (Low-Rank Adaptation)** for parameter-efficient fine-tuning, also following the setup from [`01-nemo-medchat-qa-peft.ipynb`](01-nemo-medchat-qa-peft.ipynb).\n",
    "\n",
    "- **Demonstrate the full workflow**, including loading the dataset and model, configuring the training, and evaluating the fine-tuned model's Q&A performance.\n",
    "\n",
    "**Environment Requirements:**\n",
    "- UCloud [NVIDIA NeMo Framework](https://docs.cloud.sdu.dk/Apps/nemo.html) app, Version `v25.02.01`\n",
    "- GPU-enabled session (e.g., A100, V100)\n",
    "\n",
    "> üõ†Ô∏è **Important Environment Note:**\n",
    "> This notebook is designed to run on **UCloud**, using the **NVIDIA NeMo Framewwork app, version `v25.02.01`**.\n",
    "> If you encounter unexpected errors, **double-check you are using the correct app version**, and that your session includes **GPU resources**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3295c949-d65f-4f1c-8439-96b707c2b3a4",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è **Step 1: Environment Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d91fd8-4f6a-4e11-8224-f707f3e47b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.get_device_name()\n",
    "    print(f\"‚úÖ GPU detected: {device}\")\n",
    "else:\n",
    "    raise RuntimeError(\"‚ùå No GPU detected! Ensure your UCloud session uses a GPU node.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b285d5a-d838-423b-9d6c-65add61f48ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üõ†Ô∏è **Step 2: Prepare the Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b90e3dd-f002-4337-836b-269176cc77d0",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df9602d-d7f3-4118-9504-30a139c57794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read all data into a list of dictionaries\n",
    "dataset = []\n",
    "with open(\"datasets/medal-qa_synthetic_dataset_v1.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        dataset.append(json.loads(line))\n",
    "\n",
    "# Preview the first 3 examples\n",
    "for idx, item in enumerate(dataset[:3]):\n",
    "    print(f\"Example {idx+1}:\\n{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bdc3d4-2335-47ac-955a-7f30cae6d88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29e9e38-f688-4e1b-b368-569769cfeb7e",
   "metadata": {},
   "source": [
    "### Perform train/validation/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949fa246-1f28-45c2-adbf-f2b20fe0455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('json', data_files='datasets/medal-qa_synthetic_dataset_v1.jsonl', split='train')\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "# Only keep \"question\" and \"answer\" fields\n",
    "dataset = dataset.remove_columns([col for col in dataset.column_names if col not in [\"question\", \"answer\"]])\n",
    "\n",
    "# Step 1: Split into 80% train and 20% temp (temp will be further split into validation and test)\n",
    "train_temp = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Step 2: Split temp into 50% validation and 50% test (each gets 10% of total data)\n",
    "val_test = train_temp[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "# Final splits\n",
    "train_dataset = train_temp[\"train\"]\n",
    "validation_dataset = val_test[\"train\"]\n",
    "test_dataset = val_test[\"test\"]\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Train size: {len(train_dataset)}, Validation size: {len(validation_dataset)}, Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122f903a-492c-4ef9-a72b-3763f552fcad",
   "metadata": {},
   "source": [
    "### Convert to NeMo JSONL format\n",
    "\n",
    "We leverage the `save_jsonl_preprocessed` utility function to prefix each example's `input` with a detailed instruction.\n",
    "This instruction sets the model's role and response style before presenting the actual question, improving task understanding and guiding generation.\n",
    "\n",
    "Each JSONL record will follow NeMo's expected `{input} {output}` prompt format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518b5804-0175-4718-8237-0659c7f0252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json\n",
    "from typing import List, Literal, Optional\n",
    "\n",
    "from datasets import DatasetDict, concatenate_datasets, load_dataset, load_from_disk\n",
    "from datasets.builder import DatasetGenerationError\n",
    "\n",
    "# Customized conversion with instruction prefix\n",
    "def save_jsonl_preprocessed(\n",
    "    dataset,\n",
    "    filename,\n",
    "    instruction=(\n",
    "        \"You are a board-certified medical professional and \"\n",
    "        \"a skilled communicator. Provide accurate, evidence‚Äëbased answers \"\n",
    "        \"to medical questions in clear, concise language, suitable for both \"\n",
    "        \"healthcare providers and patients.\"\n",
    "    ),\n",
    "):\n",
    "    \"\"\"\n",
    "    Writes a JSONL file where each line is:\n",
    "      {\"input\": \"<instruction> Question: ‚Ä¶\\n\\n### Response:\\n\", \"output\": \"‚Ä¶\"}\n",
    "    with *actual* newlines and one JSON record per line.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for example in dataset:\n",
    "            q = example[\"question\"]\n",
    "            a = example[\"answer\"]\n",
    "            # Use real newlines (\\n), not literal backslashes\n",
    "            inp = f\"{instruction} Question: {q}\\n\\n### Response:\\n\"\n",
    "            json.dump({\"input\": inp, \"output\": a}, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\") \n",
    "\n",
    "# after your splits:\n",
    "save_jsonl_preprocessed(train_dataset,      \"datasets/medal_train.jsonl\")\n",
    "save_jsonl_preprocessed(validation_dataset, \"datasets/medal_validation.jsonl\")\n",
    "save_jsonl_preprocessed(test_dataset,       \"datasets/medal_test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3357b3c7-dbc1-4086-8c32-090a739fcb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n3 datasets/medal_train.jsonl | jq ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f30a6b-ac58-4095-a333-ef02a9138076",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n3 datasets/medal_validation.jsonl | jq ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453bfc7d-fafb-4f4c-9576-e12a0674ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n3 datasets/medal_test.jsonl | jq ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1d887",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è **Step 3: LoRA Fine-Tuning**\n",
    "\n",
    "In this step, we‚Äôll set up and launch the LoRA fine-tuning run using NeMo‚Äôs high-level model fine-tuning script. We only need to specify a few essential parameters‚Äîdataset paths, PEFT scheme, optimizer settings, parallelism degrees, and batch sizes‚Äîto get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93614eb2-8f1f-49c0-a5bd-c455b4549f8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import Password\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "pwd = Password(description=\"Hugging Face Token:\")\n",
    "display(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afea11b5-ce7a-45e9-92a3-7f78b8207e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = pwd.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899a230d-51b2-467e-9ea2-aa59a23f08e6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$token\"\n",
    "\n",
    "# Log in to HuggingFace to get AutoTokenizer with pretrained_model_name\n",
    "HF_TOKEN=\"$1\"\n",
    "huggingface-cli login --token \"$HF_TOKEN\"\n",
    "\n",
    "# Set paths to the model, train, validation and test sets.\n",
    "PRECISION=bf16\n",
    "MODEL=\"models/llama-3.1-instruct/8B/nemo/$PRECISION/Llama-3_1-Instruct-8B.nemo\"\n",
    "\n",
    "OUTPUT_DIR=\"lora/llama-3.1-instruct-medal/8B/$PRECISION\"\n",
    "rm -rf \"$OUTPUT_DIR\"\n",
    "\n",
    "TRAIN_DS=\"['datasets/medal_train.jsonl']\"\n",
    "VALID_DS=\"['datasets/medal_validation.jsonl']\"\n",
    "\n",
    "SCHEME=\"lora\"\n",
    "GPUS=4 # Adjust if necessary\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "# Monitor training using WandB\n",
    "export WANDB_API_KEY=\"\" # Use your WandB API key\n",
    "WANDB_LOGGER=False # Set equal to True to instantiate a¬†WandB logger\n",
    "WANDB_PROJECT=\"Medal-QA\"\n",
    "\n",
    "export PYTHONWARNINGS=\"ignore\"\n",
    "\n",
    "torchrun --nproc_per_node=${GPUS} \\\n",
    "/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n",
    "    exp_manager.exp_dir=${OUTPUT_DIR} \\\n",
    "    exp_manager.explicit_log_dir=${OUTPUT_DIR} \\\n",
    "    trainer.devices=${GPUS} \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.precision=${PRECISION} \\\n",
    "    trainer.val_check_interval=213 \\\n",
    "    trainer.max_steps=2000 \\\n",
    "    exp_manager.early_stopping_callback_params.patience=3 \\\n",
    "    model.megatron_amp_O2=True \\\n",
    "    ++model.mcore_gpt=True \\\n",
    "    ++model.dist_ckpt_load_strictness=log_all \\\n",
    "    model.tensor_model_parallel_size=${TP_SIZE} \\\n",
    "    model.pipeline_model_parallel_size=${PP_SIZE} \\\n",
    "    model.global_batch_size=64 \\\n",
    "    model.micro_batch_size=16 \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    model.data.train_ds.file_names=${TRAIN_DS} \\\n",
    "    model.data.train_ds.concat_sampling_probabilities=[1.0] \\\n",
    "    model.data.validation_ds.file_names=${VALID_DS} \\\n",
    "    model.peft.peft_scheme=${SCHEME} \\\n",
    "    model.optim.name=fused_adam \\\n",
    "    model.optim.lr=5e-6 \\\n",
    "    exp_manager.create_wandb_logger=${WANDB_LOGGER} \\\n",
    "    exp_manager.wandb_logger_kwargs.project=${WANDB_PROJECT} \\\n",
    "    exp_manager.resume_if_exists=True \\\n",
    "    exp_manager.create_checkpoint_callback=True \\\n",
    "    exp_manager.checkpoint_callback_params.monitor=validation_loss \\\n",
    "    exp_manager.resume_ignore_no_checkpoint=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe41ef6-7514-4c5d-bce9-ae1daecce770",
   "metadata": {},
   "source": [
    "This will create a LoRA adapter - a file named `megatron_gpt_peft_lora_tuning.nemo` in `./lora/llama-3.1-instruct-medal/.../checkpoints/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cc422f-f654-4694-8832-b7455f397b7f",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è **Step 4: Model Evaluation**\n",
    "\n",
    "After fine-tuning and merging LoRA adapters, we evaluate the model by generating answers on the test set using NeMo's high-level model evaluation script:\n",
    "[megatron_gpt_generate.py](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py).\n",
    "\n",
    "We'll compute two metrics:\n",
    "- **Exact Match (EM)**: whether the prediction exactly matches the label.\n",
    "- **Token-level F1**: overlap between prediction and label tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a6537-f0f3-40dd-ab52-a423a85f0371",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check that the LORA model file exists\n",
    "\n",
    "python -c \"import torch; torch.cuda.empty_cache()\"\n",
    "\n",
    "PRECISION=bf16\n",
    "OUTPUT_DIR=\"lora/llama-3.1-instruct-medal/8B/$PRECISION\"\n",
    "ls -l $OUTPUT_DIR/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a2a2bb-f5ad-4077-a516-1ac6c7ad1f7c",
   "metadata": {},
   "source": [
    "In the code snippet below, the following configurations are worth noting: \n",
    "\n",
    "1. `model.restore_from_path` to the path for the `Llama-3_1-Instruct-8B.nemo` file.\n",
    "2. `model.peft.restore_from_path` to the path for the PEFT checkpoint that was created in the fine-tuning run in the last step.\n",
    "3. `model.test_ds.file_names` to the path of the `medal_test.jsonl` file.\n",
    "\n",
    "If you have made any changes in model or experiment paths, please ensure they are configured correctly below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93108124-32a5-4c8f-ab25-52dbe9b26ac6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$token\"\n",
    "\n",
    "# Log in to HuggingFace to get AutoTokenizer with pretrained_model_name\n",
    "HF_TOKEN=\"$1\"\n",
    "huggingface-cli login --token \"$HF_TOKEN\"\n",
    "\n",
    "PRECISION=bf16\n",
    "MODEL=\"models/llama-3.1-instruct/8B/nemo/$PRECISION/Llama-3_1-Instruct-8B.nemo\"\n",
    "OUTPUT_DIR=\"lora/llama-3.1-instruct-medal/8B/$PRECISION\"\n",
    "TEST_DS=\"[datasets/medal_test.jsonl]\"\n",
    "TEST_NAMES=\"[medal]\"\n",
    "SCHEME=\"lora\"\n",
    "GPUS=4 # Adjust if necessary\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "# This is where your LoRA checkpoint was saved\n",
    "PATH_TO_TRAINED_MODEL=\"$OUTPUT_DIR/checkpoints/megatron_gpt_peft_lora_tuning.nemo\"\n",
    "\n",
    "# The generation run will save the generated outputs over the test dataset in a file prefixed like so\n",
    "OUTPUT_PREFIX=\"results/medalQA_result_lora_tuning_\"\n",
    "\n",
    "export PYTHONWARNINGS=\"ignore\"\n",
    "export TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "torchrun --nproc_per_node=${GPUS} \\\n",
    "/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    model.peft.restore_from_path=${PATH_TO_TRAINED_MODEL} \\\n",
    "    trainer.devices=${GPUS} \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.precision=${PRECISION} \\\n",
    "    model.megatron_amp_O2=True \\\n",
    "    model.global_batch_size=64 \\\n",
    "    model.micro_batch_size=16 \\\n",
    "    model.data.test_ds.file_names=${TEST_DS} \\\n",
    "    model.data.test_ds.names=${TEST_NAMES} \\\n",
    "    model.data.test_ds.global_batch_size=64 \\\n",
    "    model.data.test_ds.tokens_to_generate=128 \\\n",
    "    model.tensor_model_parallel_size=${TP_SIZE} \\\n",
    "    model.pipeline_model_parallel_size=${PP_SIZE} \\\n",
    "    inference.greedy=True \\\n",
    "    model.data.test_ds.output_file_path_prefix=${OUTPUT_PREFIX} \\\n",
    "    model.data.test_ds.write_predictions_to_file=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5c0fdc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!head -n 10 results/medchatQA_result_lora_tuning__test_medal_inputs_preds_labels.jsonl | jq ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb362908-4345-4f62-a60d-0a075005259d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def compute_f1(pred: str, ref: str) -> float:\n",
    "    pred_tokens = pred.lower().split()\n",
    "    ref_tokens = ref.lower().split()\n",
    "    common = set(pred_tokens) & set(ref_tokens)\n",
    "    num_common = sum(min(pred_tokens.count(tok), ref_tokens.count(tok)) for tok in common)\n",
    "    if num_common == 0:\n",
    "        return 0.0\n",
    "    precision = num_common / len(pred_tokens)\n",
    "    recall = num_common / len(ref_tokens)\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Evaluate results from combined JSONL\n",
    "file_path = 'results/medchatQA_result_lora_tuning__test_medchat_inputs_preds_labels.jsonl'\n",
    "\n",
    "exacts = []\n",
    "f1s = []\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        pred = obj.get('pred', '').strip()\n",
    "        label = obj.get('label', '').strip()\n",
    "        exacts.append(float(pred == label))\n",
    "        f1s.append(compute_f1(pred, label))\n",
    "\n",
    "# Aggregate and display\n",
    "avg_em = sum(exacts) / len(exacts)\n",
    "avg_f1 = sum(f1s) / len(f1s)\n",
    "print(f\"Average Exact Match (EM): {avg_em:.3f}\")\n",
    "print(f\"Average F1 Score: {avg_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6453e9-cfe6-4171-849f-b1a8cc860791",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$token\"\n",
    "\n",
    "# OPTIONAL: Assess performance on the original model\n",
    "\n",
    "# Log in to HuggingFace to get AutoTokenizer with pretrained_model_name\n",
    "HF_TOKEN=\"$1\"\n",
    "huggingface-cli login --token \"$HF_TOKEN\"\n",
    "\n",
    "PRECISION=bf16\n",
    "MODEL=\"models/llama-3.1-instruct/8B/nemo/$PRECISION/Llama-3_1-Instruct-8B.nemo\"\n",
    "TEST_DS=\"[datasets/medal_test.jsonl]\"\n",
    "TEST_NAMES=\"[medal]\"\n",
    "SCHEME=\"lora\"\n",
    "GPUS=4 # Adjust if necessary\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "# The generation run will save the generated outputs over the test dataset in a file prefixed like so\n",
    "OUTPUT_PREFIX=\"results/medalQA_result_no_tuning_\"\n",
    "\n",
    "export PYTHONWARNINGS=\"ignore\"\n",
    "export TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "torchrun --nproc_per_node=${GPUS} \\\n",
    "/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    trainer.devices=${GPUS} \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.precision=${PRECISION} \\\n",
    "    model.megatron_amp_O2=True \\\n",
    "    model.global_batch_size=64 \\\n",
    "    model.micro_batch_size=16 \\\n",
    "    model.data.test_ds.file_names=${TEST_DS} \\\n",
    "    model.data.test_ds.names=${TEST_NAMES} \\\n",
    "    model.data.test_ds.global_batch_size=64 \\\n",
    "    model.data.test_ds.tokens_to_generate=128 \\\n",
    "    model.tensor_model_parallel_size=${TP_SIZE} \\\n",
    "    model.pipeline_model_parallel_size=${PP_SIZE} \\\n",
    "    inference.greedy=True \\\n",
    "    model.data.test_ds.output_file_path_prefix=${OUTPUT_PREFIX} \\\n",
    "    model.data.test_ds.write_predictions_to_file=True \\\n",
    "    model.data.test_ds.label_key='output' \\\n",
    "    model.data.test_ds.add_eos=True \\\n",
    "    model.data.test_ds.add_sep=False \\\n",
    "    model.data.test_ds.add_bos=False \\\n",
    "    model.data.test_ds.truncation_field=\"input\" \\\n",
    "    model.data.test_ds.prompt_template=\"\\{input\\} \\{output\\}\" \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d97d301-85ff-4e73-93a1-e74d230432d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 10 results/medalQA_result_no_tuning__test_medchat_inputs_preds_labels.jsonl | jq ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c66a9c4-2705-4996-9cd8-6658b9a65e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def compute_f1(pred: str, ref: str) -> float:\n",
    "    pred_tokens = pred.lower().split()\n",
    "    ref_tokens = ref.lower().split()\n",
    "    common = set(pred_tokens) & set(ref_tokens)\n",
    "    num_common = sum(min(pred_tokens.count(tok), ref_tokens.count(tok)) for tok in common)\n",
    "    if num_common == 0:\n",
    "        return 0.0\n",
    "    precision = num_common / len(pred_tokens)\n",
    "    recall = num_common / len(ref_tokens)\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Evaluate results from combined JSONL\n",
    "file_path = 'results/medalQA_result_no_tuning__test_medchat_inputs_preds_labels.jsonl'\n",
    "\n",
    "exacts = []\n",
    "f1s = []\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        pred = obj.get('pred', '').strip()\n",
    "        label = obj.get('label', '').strip()\n",
    "        exacts.append(float(pred == label))\n",
    "        f1s.append(compute_f1(pred, label))\n",
    "\n",
    "# Aggregate and display\n",
    "avg_em = sum(exacts) / len(exacts)\n",
    "avg_f1 = sum(f1s) / len(f1s)\n",
    "print(f\"Average Exact Match (EM): {avg_em:.3f}\")\n",
    "print(f\"Average F1 Score: {avg_f1:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
