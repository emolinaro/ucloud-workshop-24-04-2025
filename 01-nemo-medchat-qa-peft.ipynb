{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f74ada-05b3-43da-b6de-c7e12745c66a",
   "metadata": {},
   "source": [
    "<center>\n",
    "  <a href=\"https://escience.sdu.dk/index.php/ucloud/\">\n",
    "    <img src=\"https://escience.sdu.dk/wp-content/uploads/2020/03/logo_esc.svg\" width=\"400\" height=\"186\" />\n",
    "  </a>\n",
    "</center>\n",
    "<br>\n",
    "<p style=\"font-size: 1.2em;\">\n",
    "  This notebook was tested using <strong>NeMo Framework v25.02.01</strong> and machine type <code>u3-gpu4</code> on UCloud.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3323204-1463-4df3-8c75-5e95b6d66ba1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 01 - Llama 3.1 Fine-tuning: Building a Medical Q&A Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a773dc3-533d-49dd-b1de-0445308871d9",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this hands-on session, we'll learn **how to fine-tune a large language model** using **Parameter-Efficient Fine-Tuning (PEFT)** techniques inside the **NVIDIA NeMo Framework**, running on the **UCloud** - Interactive HPC platform.\n",
    "\n",
    "**Environment Requirements:**\n",
    "- UCloud [NVIDIA NeMo Framework](https://docs.cloud.sdu.dk/Apps/nemo.html) app, Version `v25.02.01`\n",
    "- GPU-enabled session (e.g., A100, V100)\n",
    "\n",
    "> üõ†Ô∏è **Important Environment Note:**\n",
    "> This notebook is designed to run on **UCloud**, using the **NVIDIA NeMo Framewwork app, version `v25.02.01`**.\n",
    "> If you encounter unexpected errors, **double-check you are using the correct app version**, and that your session includes **GPU resources**.\n",
    "\n",
    "**By the end of this notebook, you will be able to:**\n",
    "1. Understand PEFT and why it matters.\n",
    "2. Load a pre-trained open-source model.\n",
    "3. Apply LoRA (Low-Rank Adaptation) fine-tuning.\n",
    "4. Evaluate improvements.\n",
    "5. Save and reuse fine-tuned adapters.\n",
    "\n",
    "üìö **What is PEFT and Why Should We Care?**\n",
    "\n",
    "Large language models (like Llama, GPT) are **very expensive to fully fine-tune**:\n",
    "- Billions of parameters.\n",
    "- Gigabytes of optimizer states.\n",
    "- Massive GPU memory needs.\n",
    "\n",
    "**PEFT** solves this by **freezing most model weights** and only training **small additional parameters**.\n",
    "\n",
    "üî• **Key Advantages:**\n",
    "- **Faster training** (minutes to hours, not days).\n",
    "- **Lower memory** usage (fits on a single A100 or even smaller GPUs).\n",
    "- **Adaptable**: train different adapters for different tasks cheaply.\n",
    "\n",
    "üß© **Popular PEFT Techniques:**\n",
    "- **LoRA**: Train low-rank matrices injected into the model layers.\n",
    "- **QLoRA**: Quantize + LoRA = even smaller memory footprint.\n",
    "- **Prefix-Tuning / P-Tuning**: Learn a small prefix instead of full weights.\n",
    "\n",
    "**This tutorial focuses on LoRA**, the most widely used PEFT method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e54465b-5641-4ba7-a06b-9eb14db45a57",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è **Step 1: Environment Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ad724f-a9c0-49f9-ba3c-1bd978eb8292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.get_device_name()\n",
    "    print(f\"‚úÖ GPU detected: {device}\")\n",
    "else:\n",
    "    raise RuntimeError(\"‚ùå No GPU detected! Ensure your UCloud session uses a GPU node.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851da25a-bdec-48d5-af11-8333fd902ac5",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è **Step 2: Download and Convert Pre-trained Model from Hugging Face**\n",
    "\n",
    "We use Hugging Face's `transformers` library and NeMo conversion utilities to fetch an open-source checkpoint and convert it into NeMo `.nemo` format.\n",
    "\n",
    "**Why convert?**\n",
    "- NeMo's inference and training pipelines expect models in `.nemo` format, which bundles both the model weights and configuration in a single file.\n",
    "- Ensures compatibility with NeMo's `restore_from` and `save_to` methods for seamless loading.\n",
    "\n",
    "**Hugging Face repo:**  [Llama 3.1 8B Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a740be91-6a6b-4ffc-9168-d1395e85957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import Password\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "pwd = Password(description=\"Hugging Face Token:\")\n",
    "display(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6675274-0309-40a4-9fd3-fb7c2a0021e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token = pwd.value\n",
    "hf_model=\"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "hf_model_path=\"models/llama-3.1-instruct/8B/hf\"\n",
    "snapshot_download(\n",
    "    repo_id=hf_model,\n",
    "    local_dir=hf_model_path,\n",
    "    token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb20a9b-6c3d-40db-8407-4d83247e8a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$hf_model_path\"\n",
    "\n",
    "ls $1\n",
    "du -sh $1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219c762a-a36a-44a3-b5bd-0e436955aa07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Convert the Model in NeMo Format\n",
    "HF_MODEL=\"models/llama-3.1-instruct/8B/hf\"\n",
    "PRECISION=bf16\n",
    "NEMO_MODEL=\"models/llama-3.1-instruct/8B/nemo/$PRECISION/Llama-3_1-Instruct-8B.nemo\"\n",
    "\n",
    "export TOKENIZERS_PARALLELISM=true\n",
    "export NUMEXPR_MAX_THREADS=$(nproc)\n",
    "\n",
    "# Convert model to .nemo \n",
    "python3 -W ignore /opt/NeMo/scripts/checkpoint_converters/convert_llama_hf_to_nemo.py \\\n",
    "        --input_name_or_path \"$HF_MODEL\" \\\n",
    "        --output_path \"$NEMO_MODEL\" \\\n",
    "        --precision \"$PRECISION\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc3b178-ca86-467f-a3fa-d310d677944e",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è **Step 3: Prepare the Dataset**\n",
    "\n",
    "In this tutorial we fine-tune on the [**MedChat-QA** dataset](https://huggingface.co/datasets/ngram/medchat-qa), a Medical Question Answering corpus.\n",
    "It is available on Hugging Face under the name `medchat-qa`. \n",
    "\n",
    "This dataset consists of approximately 30000 questions, covering about 1000 FDA approved human prescription drugs.\n",
    "\n",
    "We download the full **MedChat-QA** dataset and **split** it into **train**, **validation**, and **test** sets in code.\n",
    "This ensures we have:\n",
    "- **Train set:** to fit model adapters.\n",
    "- **Validation set:** to tune hyperparameters and monitor for overfitting.\n",
    "- **Test set:** to assess final model performance on unseen data.\n",
    "\n",
    "Proper splitting prevents information leakage and ensures unbiased evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d78f50-89ea-443f-8f24-f52294479e8f",
   "metadata": {},
   "source": [
    "### Download the dataset from Hugging Face\n",
    "\n",
    "We use the `datasets` library to fetch the train and validation splits directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d2045c-519e-42f0-befd-d8958d36884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load full dataset (no predefined splits)\n",
    "dataset = load_dataset(\"ngram/medchat-qa\", split = \"train\", cache_dir=\"datasets\")\n",
    "print(f\"‚úÖ Downloaded MedChat-QA with {len(dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d8e8d9-70a0-40ec-998e-a6970cfc1234",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65b8d28-c602-4438-9aa2-e232d0e386e8",
   "metadata": {},
   "source": [
    "### Perform train/validation/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d755ff4-21fe-48c5-bf03-7accdf2ece5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ngram/medchat-qa\", split = \"train\", cache_dir=\"datasets\")\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "# Step 1: Split into 90% train and 10% temp (temp will be further split into validation and test)\n",
    "train_temp = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Step 2: Split temp into 50% validation and 50% test (each gets 5% of total data)\n",
    "val_test = train_temp[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "# Final splits\n",
    "train_dataset = train_temp[\"train\"]\n",
    "validation_dataset = val_test[\"train\"]\n",
    "test_dataset = val_test[\"test\"]\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Train size: {len(train_dataset)}, Validation size: {len(validation_dataset)}, Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb4ef45-8544-4b88-a408-0b69688c9344",
   "metadata": {},
   "source": [
    "### Convert to NeMo JSONL format\n",
    "\n",
    "We leverage the `save_jsonl_preprocessed` utility function to prefix each example's `input` with a detailed instruction.\n",
    "This instruction sets the model's role and response style before presenting the actual question, improving task understanding and guiding generation.\n",
    "\n",
    "Each JSONL record will follow NeMo's expected `{input} {output}` prompt format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612fc018-29e6-4d6c-b5e6-e2009fbc1f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json\n",
    "from typing import List, Literal, Optional\n",
    "\n",
    "from datasets import DatasetDict, concatenate_datasets, load_dataset, load_from_disk\n",
    "from datasets.builder import DatasetGenerationError\n",
    "\n",
    "# Customized conversion with instruction prefix\n",
    "def save_jsonl_preprocessed(\n",
    "    dataset,\n",
    "    filename,\n",
    "    instruction=(\n",
    "        \"You are a board-certified medical professional and \"\n",
    "        \"a skilled communicator. Provide accurate, evidence‚Äëbased answers \"\n",
    "        \"to medical questions in clear, concise language, suitable for both \"\n",
    "        \"healthcare providers and patients.\"\n",
    "    ),\n",
    "):\n",
    "    \"\"\"\n",
    "    Writes a JSONL file where each line is:\n",
    "      {\"input\": \"<instruction> Question: ‚Ä¶\\n\\n### Response:\\n\", \"output\": \"‚Ä¶\"}\n",
    "    with *actual* newlines and one JSON record per line.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for example in dataset:\n",
    "            q = example[\"question\"]\n",
    "            a = example[\"answer\"]\n",
    "            # Use real newlines (\\n), not literal backslashes\n",
    "            inp = f\"{instruction} Question: {q}\\n\\n### Response:\\n\"\n",
    "            json.dump({\"input\": inp, \"output\": a}, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\") \n",
    "\n",
    "# after your splits:\n",
    "save_jsonl_preprocessed(train_dataset,      \"datasets/medchat_train.jsonl\")\n",
    "save_jsonl_preprocessed(validation_dataset, \"datasets/medchat_validation.jsonl\")\n",
    "save_jsonl_preprocessed(test_dataset,       \"datasets/medchat_test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e445a573-76c6-41db-8e74-e7da0e8d0f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n3 datasets/medchat_train.jsonl | jq ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0bd328-f15e-446a-97d7-a020eb70d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n3 datasets/medchat_validation.jsonl | jq ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a7452b-646e-4f37-832f-16e6fe7e7270",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n3 datasets/medchat_test.jsonl | jq ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087165f8-0bd3-442b-a3d4-50fc44823fa0",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è **Step 4: LoRA Fine-Tuning**\n",
    "\n",
    "In this step, we‚Äôll set up and launch the LoRA fine-tuning run using NeMo‚Äôs high-level model fine-tuning script. We only need to specify a few essential parameters‚Äîdataset paths, PEFT scheme, optimizer settings, parallelism degrees, and batch sizes‚Äîto get started.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c145df51-3a9b-41b5-8f68-e2daab3d7b9f",
   "metadata": {},
   "source": [
    "### ü§î Understanding Fine-Tuning Objectives for LLMs\n",
    "\n",
    "Fine-tuning a large language model (LLM) like Llama 3.1 does not mean memorizing every possible answer verbatim‚Äîit means:\n",
    "\n",
    "1. **Adapting to domain-specific language and style**: The model learns terminology, phrasing, and response formats relevant to medical QA.\n",
    "2. **Improving factual consistency**: By training on medical question‚Äìanswer pairs, the model reinforces evidence-based associations.\n",
    "3. **Enhancing reasoning patterns**: Exposure to step-by-step medical explanations helps the model generalize reasoning to new questions.\n",
    "\n",
    "Since test questions may never have appeared in training, we do not expect exact matches. Instead, we measure:\n",
    "- **Content relevance**: Does the generated answer address the question accurately?\n",
    "- **Factual correctness**: Are medical facts presented correctly, even if worded differently?\n",
    "- **Clarity and completeness**: Is the response concise yet informative, following the instruction prompt?\n",
    "\n",
    "In summary, fine-tuning refines the LLM‚Äôs ability to **generalize** medical Q&A skills to unseen queries, not to retrieve memorized answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936e31d4-6e94-44e8-a085-a99becb0fc45",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ‚öôÔ∏è Data Parallelism & Gradient Accumulation\n",
    "\n",
    "In large-scale training, we often want to process more samples than a single GPU can handle at once. Two techniques help:\n",
    "\n",
    "1. **Data Parallelism:** Each GPU holds a copy of the model and processes different micro batches in parallel. After each backward pass, gradients are summed across GPUs.\n",
    "2. **Gradient Accumulation:** Instead of updating weights after every micro batch, we accumulate gradients over multiple micro batches to simulate a larger **global batch** while keeping memory usage constant.\n",
    "\n",
    "#### Visual Illustration of Accumulation\n",
    "\n",
    "```text\n",
    "‚îå‚îÄ micro‚Äëbatch #1 ‚îÄ‚î¨‚îÄ micro‚Äëbatch #2 ‚îÄ‚î¨‚îÄ micro‚Äëbatch #3 ‚îÄ‚î¨‚îÄ micro‚Äëbatch #4 ‚îÄ‚îê\n",
    "‚îÇ                  ‚îÇ                  ‚îÇ                  ‚îÇ                  ‚îÇ\n",
    "‚îÇ   forward pass   ‚îÇ   forward pass   ‚îÇ   forward pass   ‚îÇ   forward pass   ‚îÇ\n",
    "‚îÇ     (loss‚ÇÅ)      ‚îÇ     (loss‚ÇÇ)      ‚îÇ     (loss‚ÇÉ)      ‚îÇ     (loss‚ÇÑ)      ‚îÇ\n",
    "‚îÇ        ‚Üì         ‚îÇ        ‚Üì         ‚îÇ        ‚Üì         ‚îÇ        ‚Üì         ‚îÇ\n",
    "‚îÇ  backward pass   ‚îÇ  backward pass   ‚îÇ  backward pass   ‚îÇ  backward pass   ‚îÇ\n",
    "‚îÇ (‚àÇl‚ÇÅ/‚àÇŒ∏)  +=     ‚îÇ (‚àÇl‚ÇÇ/‚àÇŒ∏)  +=     ‚îÇ (‚àÇl‚ÇÉ/‚àÇŒ∏)  +=     ‚îÇ (‚àÇl‚ÇÑ/‚àÇŒ∏)  =      ‚îÇ\n",
    "‚îÇ accumulate grads ‚îÇ accumulate grads ‚îÇ accumulate grads ‚îÇ accumulate grads ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                            ‚îÇ\n",
    "                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                        ‚îÇ            optimizer step            ‚îÇ\n",
    "                        ‚îÇ      Œ∏ ‚Üê Œ∏ ‚àí lr ¬∑ Œ£¬†grads¬†/¬†64       ‚îÇ\n",
    "                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "- **Micro batch:** Number of samples processed per GPU per forward/backward pass (e.g., 16).\n",
    "- **Gradient accumulation steps:** Number of consecutive micro batches (forward/backward passes) each GPU processes‚Äîaccumulating (i.e. summing) their gradients‚Äîbefore performing a single optimizer update. \n",
    "- **Global batch:** total samples whose gradients contribute to one weight update across all GPUs; equals: `micro-batch¬†size √ó gradient¬†accumulation¬†steps √ó number¬†of¬†GPUs`.\n",
    "\n",
    "#### Example of Gradient Accumulation\n",
    "\n",
    "```text\n",
    "# Settings:\n",
    "micro_batch_size       = 16    # samples per GPU per forward/backward pass\n",
    "gradient_accumulation  = 1     # micro‚Äëbatches per weight update on each GPU\n",
    "num_GPUs               = 4\n",
    "global_batch_size      = micro_batch_size √ó gradient_accumulation √ó num_GPUs\n",
    "                       = 16 √ó 1 √ó 4\n",
    "                       = 64    # samples per weight update\n",
    "\n",
    "# Then each global_step processes one global batch (64 samples):\n",
    "global_step = 1   ‚Üí processed 1 √ó 64   =   64 samples ‚Üí 1st weight update  \n",
    "global_step = 2   ‚Üí processed 2 √ó 64   =  128 samples ‚Üí 2nd weight update  \n",
    "‚Ä¶  \n",
    "global_step = 426 ‚Üí processed 426 √ó 64 = 27264 samples ‚Üí 426th weight update (‚âà1¬†epoch)\n",
    "```\n",
    "\n",
    "#### Detailed Workflow \n",
    "| Stage                 | Description                                                                                                                                                                                   | Purpose                                                                                                  |\n",
    "|-----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|\n",
    "| Forward pass          | The network runs on input data, produces logits, and computes a scalar loss                                                                                                                   | Needed to know how wrong the current weights are                                                           |\n",
    "| Backward pass         | PyTorch¬†autograd walks the graph in reverse, computing gradients (‚àÇloss¬†/¬†‚àÇŒ∏) for every parameter¬†Œ∏                                                                                            | Gives the direction to adjust each weight                                                                  |\n",
    "| Gradient accumulation | Instead of calling `optimizer.step()` immediately, we add these gradients to a running buffer                                                                                                 | Lets us mimic a larger batch without fitting all samples in memory                                         |\n",
    "| Optimizer step        | After we have accumulated gradients from enough micro‚Äëbatches to equal the global batch size, we update the weights once (SGD,¬†Adam,¬†etc.), then zero the grad buffers                          | This is the true training step seen by the learning‚Äërate scheduler and appears in training logs/metrics    |\n",
    "\n",
    "This approach provides flexibility:\n",
    "- **Larger effective batch sizes** for stable training and better convergence.\n",
    "- **Memory efficiency** by keeping per-step memory constant.\n",
    "- **Scalability** across multiple GPUs with straightforward gradient synchronization.\n",
    "‚ÄØ264 samples ‚Üí 426th weight update (‚âà1‚ÄØepoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2918531-5a92-4a8a-9fe5-55f8054c8695",
   "metadata": {},
   "source": [
    "### üöÄ Launching the LoRA Fine-Tuning Script\n",
    "\n",
    "NeMo framework includes a high level Python script for fine-tuning ([megatron_gpt_finetuning.py](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py)) that can abstract away some of the lower level API calls. Once you have your model downloaded and the dataset ready, LoRA fine-tuning with NeMo is essentially just running this script!\n",
    "\n",
    "Some of the relevant settings are:\n",
    "\n",
    "#### Training dataset JSONL file(s)\n",
    "```bash\n",
    "model.data.train_ds.file_names='datasets/medchat_train.jsonl'\n",
    "```\n",
    "#### Validation dataset JSONL file(s)\n",
    "```bash\n",
    "model.data.validation_ds.file_names='datasets/medchat_validation.jsonl'\n",
    "```\n",
    "#### PEFT method: LoRA scheme\n",
    "```bash\n",
    "model.peft.peft_scheme=lora\n",
    "```\n",
    "#### O2-level automatic mixed precision\n",
    "```bash\n",
    "model.megatron_amp_O2=True\n",
    "```\n",
    "#### Optimizer and learning rate configuration\n",
    "```bash\n",
    "model.optim.name=fused_adam\n",
    "model.optim.lr=5e-6\n",
    "```\n",
    "#### Tensor model parallelism across model layers\n",
    "```bash\n",
    "model.tensor_model_parallel_size=1\n",
    "```\n",
    "#### Pipeline model parallelism across model stages\n",
    "```bash\n",
    "model.pipeline_model_parallel_size=1  \n",
    "```\n",
    "#### Effective batch size across all GPUs and gradient accumulation steps\n",
    "```bash\n",
    "model.global_batch_size=64 \n",
    "```\n",
    "#### Number of samples per GPU per forward/backward pass\n",
    "```bash\n",
    "model.micro_batch_size=16\n",
    "```\n",
    "For this demonstration, this training run is capped by `max_steps`, and validation is carried out every `val_check_interval` steps. If the validation loss does not improve after a few checks, training is halted to avoid overfitting.\n",
    "\n",
    "> `NOTE:` In the block of code below, pass the paths to your train and validation data files as well as path to the `.nemo` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899a230d-51b2-467e-9ea2-aa59a23f08e6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$token\"\n",
    "\n",
    "# Log in to HuggingFace to get AutoTokenizer with pretrained_model_name\n",
    "HF_TOKEN=\"$1\"\n",
    "huggingface-cli login --token \"$HF_TOKEN\"\n",
    "\n",
    "# Set paths to the model, train, validation and test sets.\n",
    "PRECISION=bf16\n",
    "MODEL=\"models/llama-3.1-instruct/8B/nemo/$PRECISION/Llama-3_1-Instruct-8B.nemo\"\n",
    "\n",
    "OUTPUT_DIR=\"lora/llama-3.1-instruct-medchat/8B/$PRECISION\"\n",
    "rm -rf \"$OUTPUT_DIR\"\n",
    "\n",
    "TRAIN_DS=\"['datasets/medchat_train.jsonl']\"\n",
    "VALID_DS=\"['datasets/medchat_validation.jsonl']\"\n",
    "\n",
    "SCHEME=\"lora\"\n",
    "GPUS=4 # Adjust if necessary\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "# Monitor training using WandB\n",
    "export WANDB_API_KEY=\"\"  # Use your WandB API key\n",
    "WANDB_LOGGER=False # Set equal to True to instantiate a¬†WandB logger\n",
    "WANDB_PROJECT=\"MedChat-QA\"\n",
    "\n",
    "export PYTHONWARNINGS=\"ignore\"\n",
    "\n",
    "torchrun --nproc_per_node=${GPUS} \\\n",
    "/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n",
    "    exp_manager.exp_dir=${OUTPUT_DIR} \\\n",
    "    exp_manager.explicit_log_dir=${OUTPUT_DIR} \\\n",
    "    trainer.devices=${GPUS} \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.precision=${PRECISION} \\\n",
    "    trainer.val_check_interval=213 \\\n",
    "    trainer.max_steps=2000 \\\n",
    "    exp_manager.early_stopping_callback_params.patience=3 \\\n",
    "    model.megatron_amp_O2=True \\\n",
    "    ++model.mcore_gpt=True \\\n",
    "    ++model.dist_ckpt_load_strictness=log_all \\\n",
    "    model.tensor_model_parallel_size=${TP_SIZE} \\\n",
    "    model.pipeline_model_parallel_size=${PP_SIZE} \\\n",
    "    model.global_batch_size=64 \\\n",
    "    model.micro_batch_size=16 \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    model.data.train_ds.file_names=${TRAIN_DS} \\\n",
    "    model.data.train_ds.concat_sampling_probabilities=[1.0] \\\n",
    "    model.data.validation_ds.file_names=${VALID_DS} \\\n",
    "    model.peft.peft_scheme=${SCHEME} \\\n",
    "    model.optim.name=fused_adam \\\n",
    "    model.optim.lr=5e-6 \\\n",
    "    exp_manager.create_wandb_logger=${WANDB_LOGGER} \\\n",
    "    exp_manager.wandb_logger_kwargs.project=${WANDB_PROJECT} \\\n",
    "    exp_manager.resume_if_exists=True \\\n",
    "    exp_manager.create_checkpoint_callback=True \\\n",
    "    exp_manager.checkpoint_callback_params.monitor=validation_loss \\\n",
    "    exp_manager.resume_ignore_no_checkpoint=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4331fd-da30-4e29-8477-3085118e4a7b",
   "metadata": {},
   "source": [
    "This will create a LoRA adapter - a file named `megatron_gpt_peft_lora_tuning.nemo` in `./lora/llama-3.1-instruct-medchat/.../checkpoints/`. We'll use this later.\n",
    "\n",
    "To further configure the run above, try the following:\n",
    "\n",
    "* **A different PEFT technique**: The `peft.peft_scheme` parameter determines the technique being used. In this case, we did LoRA, but NeMo Framework supports other techniques as well - such as P-tuning, Adapters, and IA3. For more information, refer to the [Supported PEFT Methods](https://docs.nvidia.com/nemo-framework/user-guide/latest/sft_peft/supported_methods.html). For example, for P-tuning, simply set \n",
    "    ```bash\n",
    "    model.peft.peft_scheme=\"ptuning\" # instead of \"lora\"\n",
    "    ```\n",
    "\n",
    "* **Tuning Llama 3.3 70B Instruct**: You will need 4 x H100 GPUs. Provide the path to it's `.nemo` checkpoint (similar to the download and conversion steps earlier), and change the model parallelization settings for the Llama 3.3 70B Instruct model to distribute across the GPUs.\n",
    "    ```bash\n",
    "    model.tensor_model_parallel_size=4\n",
    "    model.pipeline_model_parallel_size=1\n",
    "    ```\n",
    "\n",
    "You can override many such configurations while running the script. A full set of possible configurations is located in [NeMo Framework Github](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/tuning/conf/megatron_gpt_finetuning_config.yaml)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53979a4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üõ†Ô∏è **Step 5: Model Evaluation**\n",
    "\n",
    "After fine-tuning and merging LoRA adapters, we evaluate the model by generating answers on the test set using NeMo's high-level model evaluation script:\n",
    "[megatron_gpt_generate.py](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py).\n",
    "\n",
    "We'll compute two metrics:\n",
    "- **Exact Match (EM)**: whether the prediction exactly matches the label.\n",
    "- **Token-level F1**: overlap between prediction and label tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d1e3f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check that the LORA model file exists\n",
    "\n",
    "python -c \"import torch; torch.cuda.empty_cache()\"\n",
    "\n",
    "PRECISION=bf16\n",
    "OUTPUT_DIR=\"lora/llama-3.1-instruct-medchat/8B/$PRECISION\"\n",
    "ls -l $OUTPUT_DIR/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3430a0b0-05a0-4179-8750-151d492bb9ae",
   "metadata": {},
   "source": [
    "In the code snippet below, the following configurations are worth noting: \n",
    "\n",
    "1. `model.restore_from_path` to the path for the `Llama-3_1-Instruct-8B.nemo` file.\n",
    "2. `model.peft.restore_from_path` to the path for the PEFT checkpoint that was created in the fine-tuning run in the last step.\n",
    "3. `model.test_ds.file_names` to the path of the `pubmedqa_test.jsonl` file.\n",
    "\n",
    "If you have made any changes in model or experiment paths, please ensure they are configured correctly below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93108124-32a5-4c8f-ab25-52dbe9b26ac6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$token\"\n",
    "\n",
    "# Log in to HuggingFace to get AutoTokenizer with pretrained_model_name\n",
    "HF_TOKEN=\"$1\"\n",
    "huggingface-cli login --token \"$HF_TOKEN\"\n",
    "\n",
    "PRECISION=bf16\n",
    "MODEL=\"models/llama-3.1-instruct/8B/nemo/$PRECISION/Llama-3_1-Instruct-8B.nemo\"\n",
    "OUTPUT_DIR=\"lora/llama-3.1-instruct-medchat/8B/$PRECISION\"\n",
    "TEST_DS=\"[datasets/medchat_test.jsonl]\"\n",
    "TEST_NAMES=\"[medchat]\"\n",
    "SCHEME=\"lora\"\n",
    "GPUS=4 # Adjust if necessary\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "# This is where your LoRA checkpoint was saved\n",
    "PATH_TO_TRAINED_MODEL=\"$OUTPUT_DIR/checkpoints/megatron_gpt_peft_lora_tuning.nemo\"\n",
    "\n",
    "# The generation run will save the generated outputs over the test dataset in a file prefixed like so\n",
    "OUTPUT_PREFIX=\"results/medchatQA_result_lora_tuning_\"\n",
    "\n",
    "export PYTHONWARNINGS=\"ignore\"\n",
    "export TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "torchrun --nproc_per_node=${GPUS} \\\n",
    "/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    model.peft.restore_from_path=${PATH_TO_TRAINED_MODEL} \\\n",
    "    trainer.devices=${GPUS} \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.precision=${PRECISION} \\\n",
    "    model.megatron_amp_O2=True \\\n",
    "    model.global_batch_size=64 \\\n",
    "    model.micro_batch_size=16 \\\n",
    "    model.data.test_ds.file_names=${TEST_DS} \\\n",
    "    model.data.test_ds.names=${TEST_NAMES} \\\n",
    "    model.data.test_ds.global_batch_size=64 \\\n",
    "    model.data.test_ds.tokens_to_generate=128 \\\n",
    "    model.tensor_model_parallel_size=${TP_SIZE} \\\n",
    "    model.pipeline_model_parallel_size=${PP_SIZE} \\\n",
    "    inference.greedy=True \\\n",
    "    model.data.test_ds.output_file_path_prefix=${OUTPUT_PREFIX} \\\n",
    "    model.data.test_ds.write_predictions_to_file=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5c0fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!head -n 10 results/medchatQA_result_lora_tuning__test_medchat_inputs_preds_labels.jsonl | jq ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c25a3cd-aced-427d-8af4-d89c09cce3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def compute_f1(pred: str, ref: str) -> float:\n",
    "    pred_tokens = pred.lower().split()\n",
    "    ref_tokens = ref.lower().split()\n",
    "    common = set(pred_tokens) & set(ref_tokens)\n",
    "    num_common = sum(min(pred_tokens.count(tok), ref_tokens.count(tok)) for tok in common)\n",
    "    if num_common == 0:\n",
    "        return 0.0\n",
    "    precision = num_common / len(pred_tokens)\n",
    "    recall = num_common / len(ref_tokens)\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Evaluate results from combined JSONL\n",
    "file_path = 'results/medchatQA_result_lora_tuning__test_medchat_inputs_preds_labels.jsonl'\n",
    "\n",
    "exacts = []\n",
    "f1s = []\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        pred = obj.get('pred', '').strip()\n",
    "        label = obj.get('label', '').strip()\n",
    "        exacts.append(float(pred == label))\n",
    "        f1s.append(compute_f1(pred, label))\n",
    "\n",
    "# Aggregate and display\n",
    "avg_em = sum(exacts) / len(exacts)\n",
    "avg_f1 = sum(f1s) / len(f1s)\n",
    "print(f\"Average Exact Match (EM): {avg_em:.3f}\")\n",
    "print(f\"Average F1 Score: {avg_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318ae8f6-2afd-4338-8a57-927ff9072bad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$token\"\n",
    "\n",
    "# OPTIONAL: Assess performance on the original model\n",
    "\n",
    "# Log in to HuggingFace to get AutoTokenizer with pretrained_model_name\n",
    "HF_TOKEN=\"$1\"\n",
    "huggingface-cli login --token \"$HF_TOKEN\"\n",
    "\n",
    "PRECISION=bf16\n",
    "MODEL=\"models/llama-3.1-instruct/8B/nemo/$PRECISION/Llama-3_1-Instruct-8B.nemo\"\n",
    "TEST_DS=\"[datasets/medchat_test.jsonl]\"\n",
    "TEST_NAMES=\"[medchat]\"\n",
    "SCHEME=\"lora\"\n",
    "GPUS=4 # Adjust if necessary\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "# The generation run will save the generated outputs over the test dataset in a file prefixed like so\n",
    "OUTPUT_PREFIX=\"results/medchatQA_result_no_tuning_\"\n",
    "\n",
    "export PYTHONWARNINGS=\"ignore\"\n",
    "export TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "torchrun --nproc_per_node=${GPUS} \\\n",
    "/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    trainer.devices=${GPUS} \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.precision=${PRECISION} \\\n",
    "    model.megatron_amp_O2=True \\\n",
    "    model.global_batch_size=64 \\\n",
    "    model.micro_batch_size=16 \\\n",
    "    model.data.test_ds.file_names=${TEST_DS} \\\n",
    "    model.data.test_ds.names=${TEST_NAMES} \\\n",
    "    model.data.test_ds.global_batch_size=64 \\\n",
    "    model.data.test_ds.tokens_to_generate=128 \\\n",
    "    model.tensor_model_parallel_size=${TP_SIZE} \\\n",
    "    model.pipeline_model_parallel_size=${PP_SIZE} \\\n",
    "    inference.greedy=True \\\n",
    "    model.data.test_ds.output_file_path_prefix=${OUTPUT_PREFIX} \\\n",
    "    model.data.test_ds.write_predictions_to_file=True \\\n",
    "    model.data.test_ds.label_key='output' \\\n",
    "    model.data.test_ds.add_eos=True \\\n",
    "    model.data.test_ds.add_sep=False \\\n",
    "    model.data.test_ds.add_bos=False \\\n",
    "    model.data.test_ds.truncation_field=\"input\" \\\n",
    "    model.data.test_ds.prompt_template=\"\\{input\\} \\{output\\}\" \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f335e91a-06e8-4a60-9dfb-f980c0f469a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 10 results/medchatQA_result_no_tuning__test_medchat_inputs_preds_labels.jsonl | jq ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699fdead-dffc-4871-9d88-53583e431f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def compute_f1(pred: str, ref: str) -> float:\n",
    "    pred_tokens = pred.lower().split()\n",
    "    ref_tokens = ref.lower().split()\n",
    "    common = set(pred_tokens) & set(ref_tokens)\n",
    "    num_common = sum(min(pred_tokens.count(tok), ref_tokens.count(tok)) for tok in common)\n",
    "    if num_common == 0:\n",
    "        return 0.0\n",
    "    precision = num_common / len(pred_tokens)\n",
    "    recall = num_common / len(ref_tokens)\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Evaluate results from combined JSONL\n",
    "file_path = 'results/medchatQA_result_no_tuning__test_medchat_inputs_preds_labels.jsonl'\n",
    "\n",
    "exacts = []\n",
    "f1s = []\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        pred = obj.get('pred', '').strip()\n",
    "        label = obj.get('label', '').strip()\n",
    "        exacts.append(float(pred == label))\n",
    "        f1s.append(compute_f1(pred, label))\n",
    "\n",
    "# Aggregate and display\n",
    "avg_em = sum(exacts) / len(exacts)\n",
    "avg_f1 = sum(f1s) / len(f1s)\n",
    "print(f\"Average Exact Match (EM): {avg_em:.3f}\")\n",
    "print(f\"Average F1 Score: {avg_f1:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
